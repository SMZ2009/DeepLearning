# Transformer架构
## 论文地址
https://arxiv.org/abs/1706.03762
## 概述
本文包含对于Transformer架构机制的完整描述，在本人通读论文后进行详细梳理，整理成适合初学者阅读的方式，便于Transformer架构入门
## 前言
你需要具备的基础知识
- FNN（前馈神经网络）
- RNN（循环神经网络）
- Encoder-Decoder（编码器-解码器架构）
- Attention（注意力机制）
## 背景
### FNN
![FNN](FNN.png)
#### 解决
- 处理固定长度输入 → 固定长度输出的问题
- 建立输入特征与输出之间的非线性映射关系
#### 问题
- 数据单向传播（无记忆能力）
- 各样本之间相互独立
- 不适合处理序列数据
#### 应用
- 图像分类（如手写数字识别）
- 表格数据预测
- 回归问题
### RNN
#### 解决
- 处理序列数据中的时序依赖问题
#### 改进
- 引入“隐藏状态”作为记忆
- 允许参数共享
- 可处理变长输入
#### 问题
- 长期记忆缺失
- 输入输出长度必须一致
#### 应用
- 语言模型（预测下一个词）
- 语音识别
- 时间序列预测
### Encoder-Decoder
#### 解决
- 处理输入序列长度 ≠ 输出序列长度的问题
- 解决Seq2Seq（序列到序列）任务
#### 改进
- Encoder：把输入序列压缩成一个“语义向量”
- Decoder：根据该向量生成目标序列
#### 问题
- 对于输入序列的每一个token，输出序列的权重都相同
- 不具有注意力，无法将权重向重要内容倾斜